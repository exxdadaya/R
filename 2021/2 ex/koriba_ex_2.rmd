---
title: "Упражнение 2"
author: "Кориба Марина"
date: "01 03 2021"
output: word_document
---

```{r setup, include=FALSE}

library('knitr')
library('class')          # функция knn()
library('e1071')          # функция naiveBayes()
library('MASS')           # функция mvrnorm()
library('emdbook')        # функция dmvnorm()
knitr::opts_chunk$set(echo = TRUE)
```


Сгенерированы данные, непрерывно объясняющие переменные.

```{r, echo=FALSE}
# Генерируем данные ------------------------------------------------------------

# ядро
my.seed <- 14

#  Пример 2 ....................................................................
n <- 100               # наблюдений всего
train.percent <- 0.85  # доля обучающей выборки

# фактические значения объясняющих переменных (нормальный закон)
set.seed(my.seed)
x1 <- rnorm(20, 3.7, n = n)

set.seed(my.seed + 1)
x2 <- rnorm(50, 3.3, n = n)

# истинные дискриминирующие правила
rules <- function(x1, x2){
    ifelse((x1 > 20 & x2 < 50) | (x1 < 18 & x2 > 52), 1, 0)
}
# Конец данных примера 2 .......................................................
```


```{r, echo=FALSE}
# Отбираем наблюдения в обучающую выборку --------------------------------------
set.seed(my.seed)
inTrain <- sample(seq_along(x1), train.percent*n)
x1.train <- x1[inTrain]
x2.train <- x2[inTrain]
x1.test <- x1[-inTrain]
x2.test <- x2[-inTrain]

# используем истинные правила, чтобы присвоить фактические классы
y.train <- rules(x1.train, x2.train)
y.test <- rules(x1.test, x2.test)

# фрейм с обучающей выборкой
df.train.1 <- data.frame(x1 = x1.train, x2 = x2.train, y = y.train)
# фрейм с тестовой выборкой
df.test.1 <- data.frame(x1 = x1.test, x2 = x2.test)

```

Нарисуем обучающую выборку на графике. Сеткой точек показаны области классов, соответствующие истинным дискриминирующим правилам.


```{r, echo=FALSE}

# сохраняем получившуюся картинку
#png("plot_2.1.png", width = 600)

# для сетки (истинных областей классов): целочисленные значения x1, x2
x1.grid <- rep(seq(floor(min(x1)), ceiling(max(x1)), by = 1),
               ceiling(max(x2)) - floor(min(x2)) + 1)
x2.grid <- rep(seq(floor(min(x2)), ceiling(max(x2)), by = 1),
               each = ceiling(max(x1)) - floor(min(x1)) + 1)

# классы для наблюдений сетки
y.grid <- rules(x1.grid, x2.grid)

# фрейм для сетки
df.grid.1 <- data.frame(x1 = x1.grid, x2 = x2.grid, y = y.grid)

# цвета для графиков
cls <- c('blue', 'orange')
cls.t <- c(rgb(0, 0, 1, alpha = 0.5), rgb(1, 0.5, 0, alpha = 0.5))

# график истинных классов
plot(df.grid.1$x1, df.grid.1$x2, 
     pch = '·', col = cls[df.grid.1[, 'y'] + 1],
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, факт')
# точки фактических наблюдений
points(df.train.1$x1, df.train.1$x2,
       pch = 21, bg = cls.t[df.train.1[, 'y'] + 1], 
       col = cls.t[df.train.1[, 'y'] + 1])
#dev.off()
```
<!--![](plot_2.1.png)-->




Обучим модель наивного байесовского классификатора и оценим её точность (верность) на обучающей выборке.

Байесовский классификатор 

```{r, echo=FALSE}
#png("plot_2.2.png", width = 600)

# строим модель
nb <- naiveBayes(y ~ ., data = df.train.1)
# получаем модельные значения на обучающей выборке как классы
y.nb.train <- ifelse(predict(nb, df.train.1[, -3], 
                             type = "raw")[, 2] > 0.5, 1, 0)

# график истинных классов
plot(df.grid.1$x1, df.grid.1$x2, 
     pch = '·',  col = cls[df.grid.1[, 'y'] + 1], 
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, модель naiveBayes')
# точки наблюдений, предсказанных по модели
points(df.train.1$x1, df.train.1$x2, 
       pch = 21, bg = cls.t[y.nb.train + 1], 
       col = cls.t[y.nb.train + 1])


  # матрица неточностей на обучающей выборке
tbl <- table(y.train, y.nb.train)
tbl
      
# точность, или верность (Accuracy)
Acc <- sum(diag(tbl)) / sum(tbl)
Acc
                           
#dev.off()
```

<!--![](plot_2.2.png)-->

Как можно видеть на графике, ту часть жёлтого класса, которая расположена в левой верхней области пространства координат, модель классифицирует неверно. Таким образом, байесовская решающая граница не моделирует разрыв жёлтого класса синим. Это происходит потому, что в непрерывном случае наивный байесовский метод исходит из допущения о линейной разделимости двух классов и нормальности распределения объясняющих переменных в них. Однако в этом примере это допущение не выполняется.

Сделаем прогноз классов Y на тестовую выборку и оценим точность модели. Как можно убедиться, точность на тестовой оказывается ниже, чем на обучающей выборке. Учитывая, как ведёт себя классификатор на обучающей выборке, такой модели доверять не стоит.


```{r, echo=FALSE}
# прогноз на тестовую выборку
y.nb.test <- ifelse(predict(nb, df.test.1, type = "raw")[, 2] > 0.5, 1, 0)

# матрица неточностей на тестовой выборке
tbl <- table(y.test, y.nb.test)
tbl
Acc <- sum(diag(tbl)) / sum(tbl)
Acc
```



Построим модель kNN. Это “ленивый” классификатор, ему не требуется предварительное обучение. А ещё это непараметрический метод, и чем меньше количество ближайших соседей , тем гибче ведёт себя разделяющая граница. Метод хорошо работает с линейно неразделимыми классами.



```{r, echo=FALSE}
# Метод kNN --------------------------------------------------------------------
#  k = 3

#png("plot_2.3.png", width = 600)

# строим модель и делаем прогноз
y.knn.train <- knn(train = scale(df.train.1[, -3]), 
                   test = scale(df.train.1[, -3]),
                   cl = df.train.1$y, k = 3)

# график истинных классов
plot(df.grid.1$x1, df.grid.1$x2, 
     pch = '·', col = cls[df.grid.1[, 'y'] + 1],
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, модель kNN')
# точки наблюдений, предсказанных по модели
points(df.train.1$x1, df.train.1$x2, 
       pch = 21, bg = cls.t[as.numeric(y.knn.train)], 
       col = cls.t[as.numeric(y.knn.train)])


# матрица неточностей на обучающей выборке
tbl <- table(y.train, y.knn.train)
tbl
# точность (Accuracy)
Acc <- sum(diag(tbl)) / sum(tbl)
Acc

#dev.off()
```

<!--![](plot_2.3.png)-->


Можно видеть, что классификация обучающей выборки методом kNN не сильно отличается от фактических классов наблюдений.
Оценим также точность модели на тестовой выборке.


```{r, echo=FALSE}
# прогноз на тестовую выборку
y.knn.test <- knn(train = scale(df.train.1[, -3]), 
                 test = scale(df.test.1[, -3]),
                 cl = df.train.1$y, k = 3)

# матрица неточностей на тестовой выборке
tbl <- table(y.test, y.knn.test)
tbl

Acc <- sum(diag(tbl)) / sum(tbl)
Acc
```
Модель kNN оказалась точной на этих данных, чего не скажешь о байесовском классификаторе

**Задача**

Построить модели на данных *примера 3* с параметрами распределений, соответствующими своему варианту. На графики нанести сетку истинных классов. Определить, какой из методов срабатывает на этих данных лучше, и почему.

- n = `r n`, доля обучающей выборки: `r train.percent*100`%

- класс $$Y=0: X \sim N((4, 19),\begin{pmatrix} 7^2 &0\\0&13^2\end{pmatrix})$$

- класс $$Y=1: X \sim N((11, 12),\begin{pmatrix} 5^2 &0\\0&10^2\end{pmatrix})$$




Сгенерированы данные, непрерывно объясняющие переменные.

```{r, echo = FALSE}


# Данные примера 3 .............................................................
n <- 100               # наблюдений всего
train.percent <- 0.85  # доля обучающей выборки

# x-ы -- двумерные нормальные случайные величины
set.seed(my.seed)
class.0 <- mvrnorm(45, mu = c(4, 19), 
                   Sigma = matrix(c(49, 0, 0, 169), 2, 2, 
                                  byrow = T))

set.seed(my.seed + 1)
class.1 <- mvrnorm(55, mu = c(11, 12), 
                   Sigma = matrix(c(25, 0, 0, 100), 2, 2, 
                                  byrow = T))

# записываем x-ы в единые векторы (объединяем классы 0 и 1)
x1 <- c(class.0[, 1], class.1[, 1])
x2 <- c(class.0[, 2], class.1[, 2])

# фактические классы Y
y <- c(rep(0, nrow(class.0)), rep(1, nrow(class.1)))

# классы для наблюдений сетки
rules.mv <- function(v.x, v.mean.y0, v.mean.y1, m.sigma.y0, m.sigma.y1){
    ifelse(dmvnorm(v.x, v.mean.y0, m.sigma.y0) > 
               dmvnorm(v.x, v.mean.y1, m.sigma.y1), 0, 1)
}
# Конец данных примера 3

```

```{r, echo = FALSE}
# Отбираем наблюдения в обучающую выборку --------------------------------------
set.seed(my.seed)
inTrain <- sample(seq_along(x1), train.percent*n)
x1.train <- x1[inTrain]
x2.train <- x2[inTrain]
x1.test <- x1[-inTrain]
x2.test <- x2[-inTrain]

# используем истинные правила, чтобы присвоить фактические классы
y.train <- y[inTrain]
y.test <- y[-inTrain]

# фрейм с обучающей выборкой
df.train.1 <- data.frame(x1 = x1.train, x2 = x2.train, y = y.train)
# фрейм с тестовой выборкой
df.test.1 <- data.frame(x1 = x1.test, x2 = x2.test)
```

Нарисуем обучающую выборку на графике. Сеткой точек показаны области классов, соответствующие истинным дискриминирующим правилам. Это правило создаём, зная истинные законы распределения классов, как максимум из двух плотностей распределения (плотность многомерного закона считаем функцией dmvnorm(), классы точкам сетки присваиваем пользовательской функцией rules.mv()).

```{r, echo = FALSE}
# Рисуем обучающую выборку графике ---------------------------------------------

#png("plot_2.4.png", width = 600)

# для сетки (истинных областей классов): целочисленные значения x1, x2
x1.grid <- rep(seq(floor(min(x1)), ceiling(max(x1)), by = 1),
               ceiling(max(x2)) - floor(min(x2)) + 1)
x2.grid <- rep(seq(floor(min(x2)), ceiling(max(x2)), by = 1),
               each = ceiling(max(x1)) - floor(min(x1)) + 1)

# классы для наблюдений сетки
y.grid <- rules.mv(as.matrix(cbind(x1.grid, x2.grid)),
                   c(4, 19), c(11, 12), 
                   matrix(c(49, 0, 0, 169), 2, 2, byrow = T),
                   matrix(c(25, 0, 0, 100), 2, 2, byrow = T))

# фрейм для сетки
df.grid.1 <- data.frame(x1 = x1.grid, x2 = x2.grid, y = y.grid)

# цвета для графиков
cls <- c('blue', 'orange')
cls.t <- c(rgb(0, 0, 1, alpha = 0.5), rgb(1,0.5,0, alpha = 0.5))

# график истинных классов
plot(df.grid.1$x1, df.grid.1$x2, 
     pch = '·', col = cls[df.grid.1[, 'y'] + 1],
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, факт')
# точки фактических наблюдений
points(df.train.1$x1, df.train.1$x2,
       pch = 21, bg = cls.t[df.train.1[, 'y'] + 1], 
       col = cls.t[df.train.1[, 'y'] + 1])
#dev.off()
```

<!--![](plot_2.4.png)-->

Обучим модель наивного байесовского классификатора и оценим её точность (верность) на обучающей выборке. Поскольку объясняющие переменные для классов сгенерированы как двумерные нормальные распределения и сами классы не перекрываются, следует ожидать, что эта модель окажется точной.



### Байесовский классификатор
```{r, echo=FALSE}

#png("plot_2.5.png", width = 600)

#  наивный байес: непрерывные объясняющие переменные

# строим модель
nb <- naiveBayes(y ~ ., data = df.train.1)
# получаем модельные значения на обучающей выборке как классы
y.nb.train <- ifelse(predict(nb, df.train.1[, -3], 
                             type = "raw")[, 2] > 0.5, 1, 0)

# график истинных классов
plot(df.grid.1$x1, df.grid.1$x2, 
     pch = '·',  col = cls[df.grid.1[, 'y'] + 1], 
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, модель naiveBayes')
# точки наблюдений, предсказанных по модели
points(df.train.1$x1, df.train.1$x2, 
       pch = 21, bg = cls.t[y.nb.train + 1], 
       col = cls.t[y.nb.train + 1])
tbl <- table(y.train, y.nb.train)
tbl

Acc <- sum(diag(tbl)) / sum(tbl)
Acc

# dev.off()
```

<!--![](plot_2.5.png)-->


Точность на обучающей выборке недостаточно высокая. Сделаем прогноз классов Y на тестовую выборку и оценим точность модели.

```{r, echo=FALSE}
# прогноз на тестовую выборку
y.nb.test <- ifelse(predict(nb, df.test.1, type = "raw")[, 2] > 0.5, 1, 0)

# матрица неточностей на тестовой выборке
tbl <- table(y.test, y.nb.test)
tbl
# точность, или верность (Accuracy)
Acc <- sum(diag(tbl)) / sum(tbl)
Acc
```
Байесовский метод разделяет классы на обучающей выборке,  практически не ошибаясь, делая всего две ошибки.

Построим модель kNN. С этими данными у метода не должно возникнуть проблем, так как он не проводит чёткой границы между классами, а в каждом случае ориентируется на соседние наблюдения.

```{r, echo=FALSE}
# Метод kNN --------------------------------------------------------------------

#png("plot_2.6.png", width = 600)

#  k = 3

# строим модель и делаем прогноз
y.knn.train <- knn(train = scale(df.train.1[, -3]), 
                   test = scale(df.train.1[, -3]),
                   cl = df.train.1$y, k = 3)

# график истинных классов
plot(df.grid.1$x1, df.grid.1$x2, 
     pch = '·', col = cls[df.grid.1[, 'y'] + 1],
     xlab = 'X1', ylab = 'Y1',
     main = 'Обучающая выборка, модель kNN')
# точки наблюдений, предсказанных по модели
points(df.train.1$x1, df.train.1$x2, 
       pch = 21, bg = cls.t[as.numeric(y.knn.train)], 
       col = cls.t[as.numeric(y.knn.train)])

# матрица неточностей на обучающей выборке
tbl <- table(y.train, y.knn.train)
tbl

# точность (Accuracy)
Acc <- sum(diag(tbl)) / sum(tbl)
Acc

#dev.off()
```

<!--![](plot_2.6.png)-->

 Точность на обучающей выборке высока, но не идеальна.
Оценка точности на тестовой выборке также показывает, что модель классифицирует верно все наблюдения, кроме шести, близких к границе разделения классов.

```{r, echo=FALSE}
tbl <- table(y.train, y.nb.train)
#tbl
#tbl[2,]

#расчет чувствительности 
TRP <- tbl[2,2]/sum(tbl[2,])
#TRP

#расчет специфичности
SPC <- tbl[1,1]/sum(tbl[1,])
#SPC

#расчет ценности положительного прогноза
PPV <- tbl[2,2]/(tbl[1, 2] + tbl[2, 2])
#PPV

#расчет ценности отрицательного прогноза
NPV <- tbl[1,1]/(tbl[1, 1] + tbl[2, 1])
#NPV

#расчет доли ложноотрицательных исходов
FNR <- 1 - TRP
#FNR

#расчет доли ложных срабатываний 
FPR <- 1 - SPC
#FPR  

#расчет доли ложного обнаружения
FDR <- 1 - PPV
#FDR

#расчет корреляции Мэтьюса
MCC <- (tbl[1, 1]*tbl[2, 2] - tbl[1, 2]*tbl[2, 1])/sqrt((tbl[1, 2] + tbl[2, 2])*sum(tbl[2,])*sum(tbl[1,])*(tbl[1, 1] + tbl[2, 1]))
#MCC

fit0 <- c(TRP, SPC, PPV, NPV, FNR, FPR, FDR, MCC)


df.models <- data.frame(fit0, row.names = c('TPR', 'SPC', 'PPV', 'NPV', 'FNR', 'FPR', 'FDR', 'MMC'),stringsAsFactors = FALSE)
names(df.models) <- c('Значение характеристики')
kable(round(df.models, 2))
```

Вывод:  характеристики качества  должны быть приближены к единице, а ошибки -  минимальными. 








